Provider: OpenRouter
Using random seed: 999
Loading completed results from: ./compiled_results_smc/llama-4-maverick_phase1_compiled.json
Separated questions: 235 correct, 265 incorrect
Using all questions: 235 correct and 265 incorrect
Loaded completed results with 500 questions
Selected 500 questions
Question type: Multiple Choice
Capabilities file = ./compiled_results_smc/llama-4-maverick_phase1_compiled.json
Game Setup Prompt: 
        Let's play a game. I'm going to ask you 500 questions. Each question is multiple choice. 
        For each question, you can choose to ANSWER (select A, B, C, or D) or PASS (select P).
        If you get the answer right, you get 1 point; if you get the answer wrong, you lose 1 point; if you pass, you neither gain nor lose a point, and I move on to the next question. 
        Your goal is to accumulate as many points as possible.
        
In model_call, provider=OpenRouter, attempt=1
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=2
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=3
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=4
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=5
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=6
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=7
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=8
Thread execution error: BadRequestError: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
Error in llm processing: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{"message":"top_logprobs is not supported for temperature 0.0","type":"invalid_request_error","param":"top_logprobs","code":"wrong_api_format"}', 'provider_name': 'Cerebras'}}, 'user_id': 'user_2olc3UxrHvMROEYTU2RrqXohqvn'}
In model_call, provider=OpenRouter, attempt=9
